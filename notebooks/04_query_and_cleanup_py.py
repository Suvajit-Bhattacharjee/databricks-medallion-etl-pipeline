# -*- coding: utf-8 -*-
"""04_Query_and_Cleanup.py

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1tZLXNZE0VTWGQKodqEdnYBdJxpxaqHvh
"""

# Databricks Notebook: 04_Query_and_Cleanup.py

# Purpose:
# This notebook demonstrates how to query the final Gold layer table for analysis
# and provides essential cleanup steps to stop all active streaming jobs
# and remove the created Delta tables and checkpoint directories.
# This is crucial for managing resources and ensuring reproducibility for future runs.

from pyspark.sql import SparkSession
from pyspark.sql.functions import col, sum, count
import os

# Initialize Spark Session (already available in Databricks notebooks)
spark = SparkSession.builder.appName("QueryAndCleanup").getOrCreate()

# --- Define Paths (should match paths from 00_Setup_and_Data_Generation.py) ---
project_base_path = os.getenv("PROJECT_BASE_PATH", "/FileStore/ecommerce_sales_project_default") # Use env var
raw_input_path = f"{project_base_path}/raw_sales_data"
bronze_table_path = f"{project_base_path}/delta/bronze_sales"
silver_table_path = f"{project_base_path}/delta/silver_sales"
gold_table_path = f"{project_base_path}/delta/gold_sales_summary"

bronze_table_name = "bronze_sales_table"
silver_table_name = "silver_sales_table"
gold_table_name = "gold_sales_summary_table"

# --- Query Gold Layer Table ---
print(f"--- Querying Gold Layer Table: {gold_table_name} ---")

# Display all data in the gold table
print("\nAll records in the Gold table:")
display(spark.read.table(gold_table_name))

# Example aggregate query: Total revenue per store
print("\nTotal Revenue per Store (from Gold table):")
display(
    spark.read.table(gold_table_name)
    .groupBy("store_id")
    .agg(sum("total_revenue").alias("overall_store_revenue"))
    .orderBy(col("overall_store_revenue").desc())
)

# Example aggregate query: Top products by quantity sold
print("\nTop Products by Quantity Sold (from Gold table):")
display(
    spark.read.table(gold_table_name)
    .groupBy("product_id")
    .agg(sum("total_quantity_sold").alias("overall_product_quantity_sold"))
    .orderBy(col("overall_product_quantity_sold").desc())
    .limit(5)
)

# --- Stop All Active Streams ---
print("\n--- Stopping all active streaming queries ---")
# It's good practice to stop streams initiated by this job,
# especially if the job is short-lived or for clean testing.
for s in spark.streams.active:
    try:
        print(f"Stopping stream: {s.name} (ID: {s.id})")
        s.stop()
        s.awaitTermination(30) # Wait for stream to terminate (with timeout)
        if s.isActive:
            print(f"Warning: Stream {s.name} is still active after awaiting termination.")
    except Exception as e:
        print(f"Error stopping stream {s.name}: {e}")
print("All active streams stopped.")


# --- Clean up Delta Tables and Checkpoints ---
print("\n--- Cleaning up Delta tables and checkpoint directories ---")

# Drop tables from the Metastore
try:
    spark.sql(f"DROP TABLE IF EXISTS {bronze_table_name}")
    spark.sql(f"DROP TABLE IF EXISTS {silver_table_name}")
    spark.sql(f"DROP TABLE IF EXISTS {gold_table_name}")
    print("Delta tables dropped from metastore.")
except Exception as e:
    print(f"Error dropping tables: {e}")

# Remove underlying files and checkpoint directories from DBFS
# This ensures a clean slate for future runs and frees up storage.
try:
    dbutils.fs.rm(project_base_path, True)
    print(f"Removed all project files and checkpoints from: {project_base_path}")
except Exception as e:
    print(f"Error removing project files: {e}")

print("\nCleanup complete. The Databricks environment is ready for a fresh run.")