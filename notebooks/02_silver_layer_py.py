# -*- coding: utf-8 -*-
"""02_Silver_Layer.py

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17GP8b8WbKzYyUdlhHRhpbhvVVlgqsGmI
"""

# Databricks Notebook: 02_Silver_Layer.py

# Purpose:
# This notebook processes data from the Bronze layer, performing cleaning,
# type casting, and applying data quality rules to create a Silver layer
# Delta table. This layer typically contains cleaned and structured data
# ready for further analysis.

from pyspark.sql import SparkSession
from pyspark.sql.functions import col, current_timestamp, to_timestamp, regexp_replace, trim
from pyspark.sql.types import IntegerType, FloatType, TimestampType, StructType, StructField, StringType
import os

# Initialize Spark Session (already available in Databricks notebooks)
spark = SparkSession.builder.appName("SilverLayer").getOrCreate()

# --- Define Paths (should match paths from 00_Setup_and_Data_Generation.py) ---
project_base_path = os.getenv("PROJECT_BASE_PATH", "/FileStore/ecommerce_sales_project_default") # Use env var
bronze_table_name = "bronze_sales_table"
silver_table_path = f"{project_base_path}/delta/silver_sales"
silver_checkpoint = f"{project_base_path}/checkpoints/silver_stream"

silver_table_name = "silver_sales_table"

# --- Read from Bronze Layer (Streaming) ---
print(f"Reading streaming data from Bronze table: {bronze_table_name}")

bronze_df_stream = spark.readStream.table(bronze_table_name)

# --- Data Cleaning and Transformation for Silver Layer ---
# 1. Type Casting: Convert string types to their correct numeric/timestamp types.
#    Handle potential non-numeric/non-timestamp values by filtering or setting to null.
# 2. Basic Data Quality:
#    - Filter out rows where essential fields like transaction_id or customer_id are null.
#    - Ensure quantity and price_per_unit are positive.
# 3. Trim whitespace from string columns.
# 4. Calculate total_amount for each transaction.

silver_df = (
    bronze_df_stream
    .withColumn("transaction_id", trim(col("transaction_id")))
    .withColumn("customer_id", trim(col("customer_id")))
    .withColumn("product_id", trim(col("product_id")))
    .withColumn("store_id", trim(col("store_id")))
    .withColumn("payment_method", trim(col("payment_method")))
    .withColumn("quantity", col("quantity").cast(IntegerType())) # Cast to Integer
    .withColumn("price_per_unit", col("price_per_unit").cast(FloatType())) # Cast to Float
    .withColumn("transaction_timestamp",
                to_timestamp(col("transaction_timestamp"), "yyyy-MM-dd'T'HH:mm:ss'Z'")) # Cast to Timestamp
    .withColumn("processing_timestamp", current_timestamp()) # Add processing metadata

    # --- Data Quality Filters (Simple Expectations) ---
    # Filter out rows that do not meet basic data quality criteria for key fields
    .filter(col("transaction_id").isNotNull())
    .filter(col("customer_id").isNotNull())
    .filter(col("product_id").isNotNull())
    .filter(col("quantity").isNotNull() & (col("quantity") > 0)) # Quantity must be a positive number
    .filter(col("price_per_unit").isNotNull() & (col("price_per_unit") > 0)) # Price must be a positive number
    .filter(col("transaction_timestamp").isNotNull()) # Timestamp must be valid

    # --- Calculate Derived Fields ---
    .withColumn("total_amount", col("quantity") * col("price_per_unit"))

    # Select and reorder columns for clarity in the Silver layer
    .select(
        col("transaction_id"),
        col("customer_id"),
        col("product_id"),
        col("quantity"),
        col("price_per_unit"),
        col("total_amount"),
        col("transaction_timestamp"),
        col("store_id"),
        col("payment_method"),
        col("processing_timestamp"), # Timestamp of when this record was processed into Silver
        col("ingestion_timestamp") # Original ingestion timestamp from Bronze
    )
)

# --- Write to Silver Delta Table ---
print(f"Starting stream to write to Silver table: {silver_table_name}")

silver_query = (
    silver_df.writeStream.format("delta")
    .outputMode("append") # Append new processed records
    .option("checkpointLocation", f"{silver_checkpoint}/data_processing") # Stream progress checkpoint
    .trigger(processingTime="1 minute") # Process new data every minute
    .toTable(silver_table_name) # Register as a global table
)

print(f"Silver stream '{silver_table_name}' started and writing to: {silver_table_path}")
print("Monitor the stream progress in the Spark UI or by querying the table.")

# In a real Databricks Job, the notebook would exit here, and the stream would continue running.
# For demonstration purposes, we might wait for a bit to ensure initial data processing.
# However, for a production job orchestrated by a bundle, you typically wouldn't put `time.sleep` here.
# The job scheduler will manage task execution.
# import time
# time.sleep(30) # Wait for 30 seconds for initial micro-batch to process

print(f"\nSuccessfully configured stream for Silver Layer. Data will land in '{silver_table_name}'.")