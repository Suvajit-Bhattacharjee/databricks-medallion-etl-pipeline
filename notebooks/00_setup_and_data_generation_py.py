# -*- coding: utf-8 -*-
"""00_Setup_and_Data_Generation.py

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10W7jnOOUzxRz16GQdUWCr9vYNXUr2LUi
"""

# Databricks Notebook: 00_Setup_and_Data_Generation.py

# Purpose:
# This notebook sets up the necessary directories in DBFS for the project
# and generates mock sales data files. This makes the project self-contained
# and reproducible within a Databricks environment when orchestrated by Asset Bundles.

from pyspark.sql import SparkSession
import os

# Initialize Spark Session (already available in Databricks notebooks)
spark = SparkSession.builder.appName("DataSetup").getOrCreate()

# --- Define Paths ---
# Base path for all project data and checkpoints in DBFS.
# This value is injected as an environment variable by the Databricks Asset Bundle.
# Provide a default for local testing or if running without a bundle.
project_base_path = os.getenv("PROJECT_BASE_PATH", "/FileStore/ecommerce_sales_project_default")

# Input path for raw data (where Auto Loader will read from)
raw_input_path = f"{project_base_path}/raw_sales_data"

# Paths for Delta Lake tables (Bronze, Silver, Gold layers)
bronze_table_path = f"{project_base_path}/delta/bronze_sales"
silver_table_path = f"{project_base_path}/delta/silver_sales"
gold_table_path = f"{project_base_path}/delta/gold_sales_summary"

# Checkpoint locations for streaming jobs
bronze_checkpoint = f"{project_base_path}/checkpoints/bronze_stream"
silver_checkpoint = f"{project_base_path}/checkpoints/silver_stream"
gold_checkpoint = f"{project_base_path}/checkpoints/gold_stream"

print(f"Base project path for this run: {project_base_path}")

# --- Clean up previous runs (for fresh start) ---
print("Cleaning up previous project directories and tables (if they exist)...")
dbutils.fs.rm(project_base_path, True) # Recursive delete

# Stop any active streams from previous runs (important for reproducibility)
# This loop should ideally be in a separate cleanup step or at the start of the job.
# For simplicity in this single job, it's here.
for s in spark.streams.active:
    try:
        s.stop()
        print(f"Stopped active stream: {s.name}")
    except Exception as e:
        print(f"Could not stop stream {s.name}: {e}")

# Drop existing tables if they exist
spark.sql(f"DROP TABLE IF EXISTS bronze_sales_table")
spark.sql(f"DROP TABLE IF EXISTS silver_sales_table")
spark.sql(f"DROP TABLE IF EXISTS gold_sales_summary_table")
print("Previous tables dropped.")

# --- Create Necessary Directories ---
print(f"Creating raw data input directory: {raw_input_path}")
dbutils.fs.mkdirs(raw_input_path)
print("Directories created.")

# --- Generate Mock Data Files ---
# Data for sales_day_1.csv
sales_day_1_content = """transaction_id,customer_id,product_id,quantity,price_per_unit,transaction_timestamp,store_id,payment_method
T001,C101,P001,2,10.50,2025-06-10T09:00:00Z,S01,Credit Card
T002,C102,P003,1,25.00,2025-06-10T09:15:00Z,S02,Cash
T003,C101,P002,3,5.75,2025-06-10T09:30:00Z,S01,Debit Card
T004,C103,P001,1,10.50,2025-06-10T10:00:00Z,S03,Credit Card
T005,C102,P004,2,12.00,2025-06-10T10:45:00Z,S02,Credit Card
T006,C104,P005,1,50.00,2025-06-10T11:00:00Z,S01,Cash
T007,C101,P003,1,25.00,2025-06-10T12:00:00Z,S03,Debit Card
T008,C105,P002,4,5.75,2025-06-10T13:00:00Z,S01,Credit Card
T009,C103,P006,1,75.00,2025-06-10T14:00:00Z,S02,Credit Card
T010,C104,P001,3,10.50,2025-06-10T15:00:00Z,S01,Cash
"""

# Data for sales_day_2.csv
sales_day_2_content = """transaction_id,customer_id,product_id,quantity,price_per_unit,transaction_timestamp,store_id,payment_method
T011,C106,P007,1,120.00,2025-06-11T09:00:00Z,S04,Credit Card
T012,C107,P001,2,10.50,2025-06-11T09:30:00Z,S01,Debit Card
T013,C106,P002,1,5.75,2025-06-11T10:00:00Z,S04,Cash
T014,C101,P005,1,50.00,2025-06-11T10:30:00Z,S03,Credit Card
T015,C108,P003,2,25.00,2025-06-11T11:00:00Z,S01,Credit Card
T016,C107,P004,3,12.00,2025-06-11T11:45:00Z,S02,Cash
T017,C109,P001,1,10.50,2025-06-11T12:00:00Z,S04,Credit Card
T018,C108,P006,1,75.00,2025-06-11T13:00:00Z,S01,Debit Card
T019,C106,P001,1,10.50,2025-06-11T14:00:00Z,S03,Credit Card
T020,C102,P007,1,120.00,2025-06-11T15:00:00Z,S02,Cash
"""

# Write the data to DBFS
dbutils.fs.put(f"{raw_input_path}/sales_day_1.csv", sales_day_1_content, True)
dbutils.fs.put(f"{raw_input_path}/sales_day_2.csv", sales_day_2_content, True)

print(f"Mock data files created in: {raw_input_path}")
print("\nSetup complete. Proceed to the next notebooks for pipeline execution.")