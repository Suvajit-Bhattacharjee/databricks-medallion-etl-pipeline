# -*- coding: utf-8 -*-
"""03_Gold_Layer.py

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1NlHM4De1qNHeCYQ-QQFkrUnChY3DMxVk
"""

# Databricks Notebook: 03_Gold_Layer.py

# Purpose:
# This notebook processes data from the Silver layer, performing aggregations
# to create a Gold layer Delta table. This layer typically contains
# aggregated, business-ready data optimized for reporting and analytics.

from pyspark.sql import SparkSession
from pyspark.sql.functions import col, current_date, sum, count, countDistinct, date_trunc, current_timestamp
from pyspark.sql.window import Window
import os

# Initialize Spark Session (already available in Databricks notebooks)
spark = SparkSession.builder.appName("GoldLayer").getOrCreate()

# --- Define Paths (should match paths from 00_Setup_and_Data_Generation.py) ---
project_base_path = os.getenv("PROJECT_BASE_PATH", "/FileStore/ecommerce_sales_project_default") # Use env var
silver_table_name = "silver_sales_table"
gold_table_path = f"{project_base_path}/delta/gold_sales_summary"
gold_checkpoint = f"{project_base_path}/checkpoints/gold_stream"

gold_table_name = "gold_sales_summary_table"

# --- Read from Silver Layer (Streaming) ---
print(f"Reading streaming data from Silver table: {silver_table_name}")

silver_df_stream = spark.readStream.table(silver_table_name)

# --- Aggregation for Gold Layer ---
# Example: Daily Sales Summary by Store and Product
# We will calculate total quantity sold and total revenue per day, per store, per product.

gold_df = (
    silver_df_stream
    .withColumn("sale_date", col("transaction_timestamp").cast("date")) # Extract date part for daily aggregation
    .groupBy("sale_date", "store_id", "product_id") # Group by relevant dimensions
    .agg(
        countDistinct("transaction_id").alias("total_transactions"), # Count unique transactions
        sum("quantity").alias("total_quantity_sold"),             # Sum of quantities sold
        sum("total_amount").alias("total_revenue")                # Sum of total amounts (revenue)
    )
    .withColumn("last_aggregated_at", current_timestamp()) # Add metadata on when this aggregate was last updated
)

# --- Write to Gold Delta Table ---
# For aggregate tables, 'complete' output mode is suitable if you want to
# overwrite the entire aggregate state with each batch, showing only the latest.
# NOTE: 'complete' mode re-computes all data in each batch. For very large datasets,
# this might be inefficient. In a production scenario, for large, continuously growing
# aggregates, you might use a stateful streaming aggregation with watermarks
# or a periodic batch `MERGE INTO` operation to upsert the daily aggregates.
print(f"Starting stream to write to Gold table: {gold_table_name}")

gold_query = (
    gold_df.writeStream.format("delta")
    .outputMode("complete") # Overwrites the entire output table with results of each batch
    .option("checkpointLocation", f"{gold_checkpoint}/data_processing") # Stream progress checkpoint
    .trigger(processingTime="1 minute") # Re-calculate and write aggregates every minute
    .toTable(gold_table_name) # Register as a global table
)

print(f"Gold stream '{gold_table_name}' started and writing to: {gold_table_path}")
print("Monitor the stream progress in the Spark UI or by querying the table.")

# In a real Databricks Job, the notebook would exit here, and the stream would continue running.
# For demonstration purposes, we might wait for a bit to ensure initial data processing.
# However, for a production job orchestrated by a bundle, you typically wouldn't put `time.sleep` here.
# The job scheduler will manage task execution.
# import time
# time.sleep(30) # Wait for 30 seconds for initial micro-batch to process

print(f"\nSuccessfully configured stream for Gold Layer. Data will land in '{gold_table_name}'.")