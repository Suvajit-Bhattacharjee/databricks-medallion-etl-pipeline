# -*- coding: utf-8 -*-
"""01_Bronze_Layer.py

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1E9ztRZVoBQKzWWpFtmrAKGhjSiUlrndf
"""

# Databricks Notebook: 01_Bronze_Layer.py

# Purpose:
# This notebook ingests raw sales data from cloud storage (simulated by DBFS)
# into a Delta Lake table in the Bronze layer. It uses Databricks Auto Loader
# for efficient, incremental data ingestion and schema inference.

from pyspark.sql import SparkSession
from pyspark.sql.functions import col, current_timestamp
from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType, TimestampType
import os

# Initialize Spark Session (already available in Databricks notebooks)
spark = SparkSession.builder.appName("BronzeLayer").getOrCreate()

# --- Define Paths (should match paths from 00_Setup_and_Data_Generation.py) ---
project_base_path = os.getenv("PROJECT_BASE_PATH", "/FileStore/ecommerce_sales_project_default") # Use env var
raw_input_path = f"{project_base_path}/raw_sales_data"
bronze_table_path = f"{project_base_path}/delta/bronze_sales"
bronze_checkpoint = f"{project_base_path}/checkpoints/bronze_stream"

bronze_table_name = "bronze_sales_table"

# --- Define Schema for Raw Data ---
# It's good practice to define the schema explicitly for streaming sources
# even if Auto Loader can infer. This provides robustness.
# All fields are read as StringType in Bronze to capture raw data as-is,
# handling potential malformed data gracefully before validation.
raw_sales_schema = StructType([
    StructField("transaction_id", StringType(), True),
    StructField("customer_id", StringType(), True),
    StructField("product_id", StringType(), True),
    StructField("quantity", StringType(), True),         # Read as String to handle potential bad data
    StructField("price_per_unit", StringType(), True),   # Read as String
    StructField("transaction_timestamp", StringType(), True), # Read as String
    StructField("store_id", StringType(), True),
    StructField("payment_method", StringType(), True)
])

# --- Ingest Data to Bronze Layer using Auto Loader ---
print(f"Starting Auto Loader to ingest data from {raw_input_path} to Bronze layer...")

bronze_df = (
    spark.readStream.format("cloudFiles")
    .option("cloudFiles.format", "csv")
    .option("cloudFiles.schemaLocation", f"{bronze_checkpoint}/schema") # Auto Loader schema inference checkpoint
    .option("header", "true") # CSV has a header row
    .schema(raw_sales_schema) # Apply the defined schema
    .load(raw_input_path)
    .withColumn("ingestion_timestamp", current_timestamp()) # Add ingestion metadata (timestamp of data arrival)
)

# Write to Bronze Delta table
# Use 'append' mode for continuous ingestion
# A checkpoint location is crucial for fault tolerance in streaming
bronze_query = (
    bronze_df.writeStream.format("delta")
    .outputMode("append")
    .option("checkpointLocation", f"{bronze_checkpoint}/data_processing") # Stream progress checkpoint
    .trigger(processingTime="1 minute") # Process new data every minute
    .toTable(bronze_table_name) # Register as a global table for easy querying
)

print(f"Bronze stream '{bronze_table_name}' started and writing to: {bronze_table_path}")
print("Monitor the stream progress in the Spark UI or by querying the table.")

# In a real Databricks Job, the notebook would exit here, and the stream would continue running.
# For demonstration purposes, we might wait for a bit to ensure initial data processing.
# However, for a production job orchestrated by a bundle, you typically wouldn't put `time.sleep` here.
# The job scheduler will manage task execution.
# import time
# time.sleep(30) # Wait for 30 seconds to allow initial micro-batch to process

print(f"\nSuccessfully configured stream for Bronze Layer. Data will land in '{bronze_table_name}'.")